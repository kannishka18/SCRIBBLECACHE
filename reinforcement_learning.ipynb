{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMZOoA49FOIcoQJF3Z3vqcl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"BBLY5zC-uwlo","executionInfo":{"status":"ok","timestamp":1744389943812,"user_tz":-330,"elapsed":729,"user":{"displayName":"Meyyappan Dm","userId":"08510943047907577662"}}},"outputs":[],"source":["import numpy as np\n","import random\n","import pickle\n","\n","class RLAgent:\n","    def __init__(self, state_space_size, action_space_size, alpha=0.1,\n","                 gamma=0.9, epsilon=0.1):\n","        self.q_table = np.zeros((state_space_size, action_space_size))\n","        self.alpha = alpha\n","        self.gamma = gamma\n","        self.epsilon = epsilon\n","        self.state_space_size = state_space_size\n","        self.action_space_size = action_space_size\n","\n","    def choose_action(self, state_idx):\n","        if random.uniform(0, 1) < self.epsilon:\n","            return random.randint(0, self.action_space_size - 1)\n","        return np.argmax(self.q_table[state_idx])\n","\n","    def update_q(self, state_idx, action_idx, reward, next_state_idx):\n","        predict = self.q_table[state_idx, action_idx]\n","        target = reward + self.gamma * np.max(self.q_table[next_state_idx])\n","        self.q_table[state_idx, action_idx] += self.alpha * (target - predict)\n","\n","    def save(self, path='q_table.pkl'):\n","        with open(path, 'wb') as f:\n","            pickle.dump(self.q_table, f)\n","\n","    def load(self, path='q_table.pkl'):\n","        with open(path, 'rb') as f:\n","            self.q_table = pickle.load(f)\n"]},{"cell_type":"code","source":["import random\n","\n","# Define states: (accuracy_level, speed_level)\n","states = [\n","    (0, 0), (0, 1), (0, 2),\n","    (1, 0), (1, 1), (1, 2),\n","    (2, 0), (2, 1), (2, 2)\n","]\n","state_to_index = {s: i for i, s in enumerate(states)}\n","\n","# 5 Actions: feedback types\n","actions = [\"text\", \"visual\", \"audio\", \"simplify\", \"increase\"]\n","\n","def simulate_user_response(state, action):\n","    \"\"\"\n","    Simulate if user's accuracy improves based on current state and action\n","    Returns new state and reward\n","    \"\"\"\n","    acc, speed = state\n","\n","    # Logic: simplify helps if acc is low, increase helps if acc is high\n","    if action == \"simplify\" and acc == 0:\n","        acc += 1\n","        reward = 1\n","    elif action == \"increase\" and acc == 2:\n","        acc = max(0, acc - 1)\n","        reward = -1\n","    elif action in [\"visual\", \"audio\"] and random.random() > 0.4:\n","        acc = min(2, acc + 1)\n","        reward = 1\n","    else:\n","        reward = -1  # no improvement\n","\n","    # Slight random change in speed\n","    speed = min(2, max(0, speed + random.choice([-1, 0, 1])))\n","    return (acc, speed), reward\n"],"metadata":{"id":"b1PhdP8Fv6Ys","executionInfo":{"status":"ok","timestamp":1744390049116,"user_tz":-330,"elapsed":511,"user":{"displayName":"Meyyappan Dm","userId":"08510943047907577662"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ksEcvpaEwNL8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from IPython.display import display\n","\n","import random\n","\n","agent = RLAgent(state_space_size=len(states), action_space_size=len(actions))\n","\n","# Train for 1000 episodes\n","for episode in range(1000):\n","    state = random.choice(states)\n","    state_idx = state_to_index[state]\n","\n","    action_idx = agent.choose_action(state_idx)\n","    action = actions[action_idx]\n","\n","    new_state, reward = simulate_user_response(state, action)\n","    new_state_idx = state_to_index[new_state]\n","\n","    agent.update_q(state_idx, action_idx, reward, new_state_idx)\n","\n","    if episode % 100 == 0:\n","        print(f\"Episode {episode} | State: {state} | Action: {action} | Reward: {reward}\")\n","\n","agent.save(\"q_table.pkl\")\n","print(\"Q-table saved!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m_Ya-udiwD_E","executionInfo":{"status":"ok","timestamp":1744390181932,"user_tz":-330,"elapsed":680,"user":{"displayName":"Meyyappan Dm","userId":"08510943047907577662"}},"outputId":"47a9550a-21d6-442e-889a-4f81c331deed"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode 0 | State: (2, 2) | Action: text | Reward: -1\n","Episode 100 | State: (1, 0) | Action: audio | Reward: 1\n","Episode 200 | State: (0, 0) | Action: simplify | Reward: 1\n","Episode 300 | State: (0, 1) | Action: visual | Reward: -1\n","Episode 400 | State: (2, 2) | Action: audio | Reward: -1\n","Episode 500 | State: (1, 0) | Action: audio | Reward: -1\n","Episode 600 | State: (0, 2) | Action: simplify | Reward: 1\n","Episode 700 | State: (2, 0) | Action: audio | Reward: 1\n","Episode 800 | State: (0, 2) | Action: simplify | Reward: 1\n","Episode 900 | State: (1, 1) | Action: visual | Reward: -1\n","Q-table saved!\n"]}]}]}